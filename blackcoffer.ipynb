{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the Excel file into a pandas DataFrame\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to scrape text from a given URL\n",
    "def scrape_text_from_url(url):\n",
    "    try:\n",
    "        # Send an HTTP request to the URL\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Extract text content from HTML (modify as needed)\n",
    "            text = soup.get_text()\n",
    "\n",
    "            return text\n",
    "        else:\n",
    "            return None  # Return None if the request was not successful\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while scraping {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through the DataFrame and scrape text from URLs\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Call the function to scrape text from the URL\n",
    "    scraped_text = scrape_text_from_url(url)\n",
    "\n",
    "    # Save the scraped text or process it as needed (e.g., text analysis)\n",
    "\n",
    "    # For example, you can save the text to a file\n",
    "    if scraped_text:\n",
    "        with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(scraped_text)\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download NLTK stopwords data (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the Excel file containing scraped data\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to perform word frequency analysis\n",
    "def word_frequency_analysis(text):\n",
    "    # Tokenize the text\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords (common words like \"the,\" \"and,\" etc.)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Calculate word frequencies\n",
    "    word_freq = Counter(words)\n",
    "\n",
    "    return word_freq\n",
    "\n",
    "# Create a folder to save the results\n",
    "output_folder = 'word_frequency_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame and perform word frequency analysis\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Read the scraped text from the file\n",
    "    text_file = f'{url_id}.txt'\n",
    "    if os.path.isfile(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            scraped_text = file.read()\n",
    "\n",
    "        # Perform word frequency analysis\n",
    "        word_freq = word_frequency_analysis(scraped_text)\n",
    "\n",
    "        # Save the word frequency results to a CSV file\n",
    "        output_file = os.path.join(output_folder, f'{url_id}_word_frequency.csv')\n",
    "        df_word_freq = pd.DataFrame(word_freq.items(), columns=['Word', 'Frequency'])\n",
    "        df_word_freq.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f'Word frequency analysis for {url_id} completed and saved to {output_file}')\n",
    "    else:\n",
    "        print(f'No scraped text found for {url_id}')\n",
    "\n",
    "print('Word frequency analysis process completed.')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Load the Excel file containing scraped data\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to perform sentiment analysis\n",
    "def sentiment_analysis(text):\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Analyze sentiment\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "    # Determine sentiment based on the compound score\n",
    "    sentiment = 'Neutral'\n",
    "    if sentiment_scores['compound'] > 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_scores['compound'] < -0.05:\n",
    "        sentiment = 'Negative'\n",
    "\n",
    "    return sentiment, sentiment_scores\n",
    "\n",
    "# Create a folder to save the sentiment analysis results\n",
    "output_folder = 'sentiment_analysis_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame and perform sentiment analysis\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Read the scraped text from the file\n",
    "    text_file = f'{url_id}.txt'\n",
    "    if os.path.isfile(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            scraped_text = file.read()\n",
    "\n",
    "        # Perform sentiment analysis\n",
    "        sentiment, sentiment_scores = sentiment_analysis(scraped_text)\n",
    "\n",
    "        # Save the sentiment analysis results to a CSV file\n",
    "        output_file = os.path.join(output_folder, f'{url_id}_sentiment_analysis.csv')\n",
    "        df_sentiment = pd.DataFrame({'Sentiment': [sentiment], **sentiment_scores})\n",
    "        df_sentiment.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f'Sentiment analysis for {url_id} completed and saved to {output_file}')\n",
    "    else:\n",
    "        print(f'No scraped text found for {url_id}')\n",
    "\n",
    "print('Sentiment analysis process completed.')\n",
    "\n",
    "!pip install nltk gensim pyLDAvis\n",
    "\n",
    "!pip install bert-extractive-summarizer\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from summarizer import Summarizer\n",
    "\n",
    "# Download NLTK stopwords data (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the Excel file containing scraped data\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to preprocess text and generate a summary\n",
    "def generate_summary(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Join the sentences back into a single text block\n",
    "    text = ' '.join(sentences)\n",
    "\n",
    "    # Generate a summary using BERT-based summarization\n",
    "    summarizer = Summarizer()\n",
    "    summary = summarizer(text)\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Create a folder to save the summary results\n",
    "output_folder = 'text_summarization_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame and generate summaries\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Read the scraped text from the file\n",
    "    text_file = f'{url_id}.txt'\n",
    "    if os.path.isfile(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            scraped_text = file.read()\n",
    "\n",
    "        # Generate a summary\n",
    "        summary = generate_summary(scraped_text)\n",
    "\n",
    "        # Save the summary to a text file\n",
    "        output_file = os.path.join(output_folder, f'{url_id}_summary.txt')\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(summary)\n",
    "\n",
    "        print(f'Summary for {url_id} generated and saved to {output_file}')\n",
    "    else:\n",
    "        print(f'No scraped text found for {url_id}')\n",
    "\n",
    "print('Text summarization process completed.')\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the Excel file containing scraped data\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to perform named entity recognition\n",
    "def perform_ner(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract named entities (e.g., persons, organizations, locations)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return entities\n",
    "\n",
    "# Create a folder to save the NER results\n",
    "output_folder = 'ner_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame and perform NER\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Read the scraped text from the file\n",
    "    text_file = f'{url_id}.txt'\n",
    "    if os.path.isfile(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            scraped_text = file.read()\n",
    "\n",
    "        # Perform NER on the text\n",
    "        entities = perform_ner(scraped_text)\n",
    "\n",
    "        # Save the NER results to a CSV file\n",
    "        output_file = os.path.join(output_folder, f'{url_id}_ner_results.csv')\n",
    "        df_entities = pd.DataFrame(entities, columns=['Entity', 'Label'])\n",
    "        df_entities.to_csv(output_file, index=False)\n",
    "\n",
    "        print(f'NER for {url_id} completed and saved to {output_file}')\n",
    "    else:\n",
    "        print(f'No scraped text found for {url_id}')\n",
    "\n",
    "print('Named Entity Recognition (NER) process completed.')\n",
    "\n",
    "!pip install rake-nltk\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the Excel file containing scraped data\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to extract keywords from text\n",
    "def extract_keywords(text):\n",
    "    # Initialize the Rake keyword extractor\n",
    "    r = Rake()\n",
    "\n",
    "    # Extract keywords from the text\n",
    "    r.extract_keywords_from_text(text)\n",
    "\n",
    "    # Get the ranked keywords\n",
    "    ranked_keywords = r.get_ranked_phrases()\n",
    "\n",
    "    return ranked_keywords\n",
    "\n",
    "# Create a folder to save the keyword extraction results\n",
    "output_folder = 'keyword_extraction_results'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Iterate through the DataFrame and extract keywords\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Read the scraped text from the file\n",
    "    text_file = f'{url_id}.txt'\n",
    "    if os.path.isfile(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            scraped_text = file.read()\n",
    "\n",
    "        # Extract keywords from the text\n",
    "        keywords = extract_keywords(scraped_text)\n",
    "\n",
    "        # Save the keywords to a text file\n",
    "        output_file = os.path.join(output_folder, f'{url_id}_keywords.txt')\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(keywords))\n",
    "\n",
    "        print(f'Keyword extraction for {url_id} completed and saved to {output_file}')\n",
    "    else:\n",
    "        print(f'No scraped text found for {url_id}')\n",
    "\n",
    "print('Keyword extraction process completed.')\n",
    "\n",
    "!pip install pandas openpyxl textblob nltk\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load the input data\n",
    "input_file = 'Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "# Create a function to perform text analysis and compute variables\n",
    "def perform_text_analysis(text):\n",
    "    # Tokenize the text into words and sentences\n",
    "    words = word_tokenize(text)\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Compute variables\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "\n",
    "    # Calculate average sentence length\n",
    "    avg_sentence_length = word_count / sentence_count\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Calculate percentage of complex words\n",
    "    complex_word_count = len([word for word in filtered_words if len(word) > 3])  # Define complex words as words with more than 3 characters\n",
    "    percentage_complex_words = (complex_word_count / word_count) * 100\n",
    "\n",
    "    # Calculate FOG Index\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Calculate average number of words per sentence\n",
    "    avg_words_per_sentence = word_count / sentence_count\n",
    "\n",
    "    # Calculate syllables per word (approximation)\n",
    "    syllable_count = sum([len(list(filter(str.isalpha, word))) for word in words])  # Count alphabetic characters as syllables\n",
    "    syllable_per_word = syllable_count / word_count\n",
    "\n",
    "    # Calculate personal pronoun count\n",
    "    personal_pronouns = ['I', 'me', 'my', 'mine', 'myself', 'we', 'us', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves']\n",
    "    personal_pronoun_count = len([word for word in words if word.lower() in personal_pronouns])\n",
    "\n",
    "    # Calculate average word length\n",
    "    avg_word_length = sum(len(word) for word in words) / word_count\n",
    "\n",
    "    # Perform sentiment analysis\n",
    "    blob = TextBlob(text)\n",
    "    polarity_score = blob.sentiment.polarity\n",
    "    subjectivity_score = blob.sentiment.subjectivity\n",
    "    positive_score = len([sent for sent in blob.sentences if sent.sentiment.polarity > 0])\n",
    "    negative_score = len([sent for sent in blob.sentences if sent.sentiment.polarity < 0])\n",
    "\n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_per_word, personal_pronoun_count, avg_word_length\n",
    "\n",
    "# Create a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through the DataFrame and perform text analysis\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "\n",
    "    # Read the scraped text from the file\n",
    "    text_file = f'{url_id}.txt'\n",
    "    if os.path.isfile(text_file):\n",
    "        with open(text_file, 'r', encoding='utf-8') as file:\n",
    "            scraped_text = file.read()\n",
    "\n",
    "        # Perform text analysis on the text\n",
    "        analysis_result = perform_text_analysis(scraped_text)\n",
    "\n",
    "        # Create a dictionary to store the results\n",
    "        result_dict = {\n",
    "            'URL_ID': url_id,\n",
    "            'URL': url,\n",
    "            'POSITIVE SCORE': analysis_result[0],\n",
    "            'NEGATIVE SCORE': analysis_result[1],\n",
    "            'POLARITY SCORE': analysis_result[2],\n",
    "            'SUBJECTIVITY SCORE': analysis_result[3],\n",
    "            'AVG SENTENCE LENGTH': analysis_result[4],\n",
    "            'PERCENTAGE OF COMPLEX WORDS': analysis_result[5],\n",
    "            'FOG INDEX': analysis_result[6],\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': analysis_result[7],\n",
    "            'COMPLEX WORD COUNT': analysis_result[8],\n",
    "            'WORD COUNT': analysis_result[9],\n",
    "            'SYLLABLE PER WORD': analysis_result[10],\n",
    "            'PERSONAL PRONOUNS': analysis_result[11],\n",
    "            'AVG WORD LENGTH': analysis_result[12]\n",
    "        }\n",
    "\n",
    "        results.append(result_dict)\n",
    "\n",
    "# Create a DataFrame from the results and save it to an Excel file\n",
    "output_df = pd.DataFrame(results)\n",
    "output_file = 'Output Data .xlsx'\n",
    "output_df.to_excel(output_file, index=False, engine='openpyxl')\n",
    "\n",
    "print('Text analysis and variable computation process completed. Results saved to', output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
